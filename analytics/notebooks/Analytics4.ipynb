{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# Required imports\n",
    "import json\n",
    "import re\n",
    "import zipfile as zp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import random\n",
    "import pygal\n",
    "import user_agents\n",
    "#flask example (example.zip musi być w katalogu głównym)\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from preprocess import preprocess, _aux_get_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_path = \"/home/jacek/Pobrane/Okon_Slowacki_30_11_2020.zip\"\n",
    "# zip_path = \"/home/jacek/Pobrane/facebook-jacekstasiak75 222.zip\"\n",
    "# zip_path = \"/home/jacek/Pobrane/facebook-jacekstasiak75.zip\"\n",
    "# zip_path = \"/home/jacek/Pobrane/win10 unpacked/facebook-okoń 28-12-20.zip\"\n",
    "# zip_path = \"/home/jacek/Pobrane/facebook-janpodkowa98.zip\"\n",
    "\n",
    "data = None\n",
    "acc_activity = None\n",
    "with zp.ZipFile(zip_path) as zip_file:\n",
    "#     data = gen_pandas_table(zip_file)\n",
    "    data = preprocess(zip_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_cum_graph(time_events):\n",
    "    a = time_events.groupby([time_events.dt.year, time_events.dt.dayofyear])\n",
    "    t = pd.Series(pd.date_range(time_events.dt.date.min(), time_events.dt.date.max()))\n",
    "    t = pd.date_range(time_events.min(), time_events.max())\n",
    "    t = pd.Series(t.date)\n",
    "    days_not_present = t.isin(time_events.dt.date)\n",
    "    days_not_present = ~days_not_present\n",
    "    days_not_present\n",
    "    # t[days_with_frens]\n",
    "    temp = pd.DataFrame(\n",
    "        {\n",
    "            'time': time_events,\n",
    "            'value': [1] * len(time_events)# + [-1] * len(days_not_present == False)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    temp = pd.concat([\n",
    "        temp, \n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                'time': t[days_not_present],\n",
    "                'value': [-1] * len(t[days_not_present])\n",
    "            }\n",
    "        )    \n",
    "    ])\n",
    "    temp = temp.sort_values(['time']).reset_index(drop=True)\n",
    "    temp\n",
    "\n",
    "    timespan = temp.time.max() - temp.time.min()\n",
    "    time_index = []\n",
    "\n",
    "    temp.time = pd.to_datetime(temp.time)\n",
    "\n",
    "    # grouped = temp.groupby(temp.time.dt.year)\n",
    "\n",
    "    # Grupowanie przedziałów zliczania liczby znajomych, w zależności od tego jak stare jest konto\n",
    "    if timespan.days > 360 * 10:\n",
    "        distance = 'Y'\n",
    "        grouped = temp.groupby([temp.time.dt.year, temp.time.dt.year])\n",
    "        time_index = pd.date_range(temp.time.min().date(), temp.time.max(), freq='Y')\n",
    "    elif timespan.days > 360 * 2:\n",
    "        distance = 'QS'\n",
    "        grouped = temp.groupby([temp.time.dt.year, temp.time.dt.quarter])\n",
    "        time_index = pd.date_range(temp.time.min().date(), temp.time.max(), freq='QS')\n",
    "    elif timespan.days > 40:\n",
    "        distance = 'W'\n",
    "        grouped = temp.groupby([temp.time.dt.year, temp.time.dt.isocalendar().week])\n",
    "        time_index = pd.date_range(temp.time.min().date(), temp.time.max(), freq='W')\n",
    "    elif timespan.days > 90:\n",
    "        distance = 'M'\n",
    "        grouped = temp.groupby([temp.time.dt.year, temp.time.dt.month])\n",
    "        time_index = pd.date_range(temp.time.min().date(), temp.time.max(), freq='M')\n",
    "    else:\n",
    "        grouped = temp.groupby([temp.time.dt.year, temp.time.dt.day])\n",
    "        time_index = pd.date_range(temp.time.min().date(), temp.time.max(), freq='D')\n",
    "\n",
    "    # zliczanie elementów które nie są '-1'\n",
    "    b = grouped['value'].agg(lambda x: len(x[x != -1]))\n",
    "\n",
    "    # cum sumowanie\n",
    "    b = b.cumsum()\n",
    "\n",
    "    return b, time_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likes and reactions\n",
    "\n",
    "like_table = pd.DataFrame({'time': [], 'reaction': []})\n",
    "with zp.ZipFile(zip_path) as zip_file:\n",
    "    folders = _aux_get_structure(zip_file)\n",
    "    \n",
    "#     print('likes_and_reactions/posts_and_comments.json' in zip_file)\n",
    "#     print(dir(zip_file))\n",
    "    paths = ['likes_and_reactions/posts_and_comments.json', 'likes_and_reactions/pages.json']\n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Likes on posts\n",
    "            with zip_file.open('likes_and_reactions/posts_and_comments.json') as f:\n",
    "                jdata = json.loads(f.read())\n",
    "\n",
    "                likes = []\n",
    "                times = []\n",
    "\n",
    "                for i in jdata['reactions']:\n",
    "                    times.append(i['timestamp'])\n",
    "                    likes.append(i['data'][0]['reaction']['reaction'])\n",
    "\n",
    "                temp_table = pd.DataFrame({'time': times, 'type': likes})\n",
    "                temp_table.time = pd.to_datetime(temp_table.time, unit='s')\n",
    "                like_table = pd.concat([like_table, temp_table])\n",
    "        except KeyError as e:\n",
    "            print(\"No likes or reactions\")\n",
    "        \n",
    "#     try:\n",
    "#         # Likes on pages\n",
    "#         with zip_file.open('likes_and_reactions/pages.json') as f:\n",
    "#             jdata = json.loads(f.read())\n",
    "\n",
    "#             likes = []\n",
    "#             times = []\n",
    "\n",
    "#             for i in jdata['page_likes']:\n",
    "#                 times.append(i['timestamp'])\n",
    "#                 likes.append(\"LIKE\")\n",
    "\n",
    "#             # Tymczasowa tabela, żeby zamienić czas na datę\n",
    "#             temp_table = pd.DataFrame({'time': times, 'type': likes})\n",
    "#             temp_table.time = pd.to_datetime(temp_table.time, unit='s')\n",
    "#             like_table = pd.concat([like_table, temp_table])\n",
    "#     except KeyError as e:\n",
    "#         print(\"No page likes\")\n",
    "like_table\n",
    "\n",
    "# cum chart\n",
    "vals, index = get_cum_graph(like_table['time'])\n",
    "char = pygal.Line(fill=True, x_label_rotation=-45, show_legend=False)\n",
    "char.x_labels = index.date\n",
    "char.add('', list(vals))\n",
    "\n",
    "name_to_e = {\n",
    "    'ANGER': '😡',\n",
    "    'HAHA': '😂',\n",
    "    'LIKE': '👍',\n",
    "    'LOVE': '❤️',\n",
    "    'SORRY': '😢',\n",
    "    'WOW': '😯'\n",
    "}\n",
    "\n",
    "by_type = like_table.groupby(like_table.type).count().time\n",
    "\n",
    "indexes = [i + ' ' + name_to_e[i] for i in by_type.index]\n",
    "\n",
    "chart = pygal.Pie()\n",
    "for i, t in zip(indexes, list(by_type)):\n",
    "    chart.add(i, t)\n",
    "chart\n",
    "\n",
    "# reszta tak jak w postach i likach\n",
    "\n",
    "# # graph by years\n",
    "# posts_by_year = like_table.groupby(like_table.time.dt.year).count()\n",
    "# year_values = list(posts_by_year.time)\n",
    "# year_indexes = list(posts_by_year.index)\n",
    "\n",
    "# if len(posts_by_year) == 1:\n",
    "#     year_values = [0] + year_values\n",
    "#     year_indexes = [year_indexes[0] - 1] + year_indexes\n",
    "\n",
    "# chart = pygal.Line(show_legend=False)\n",
    "# chart.x_labels = year_indexes\n",
    "# chart.add('', year_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change\n",
    "# Emoji dalej nie działa 🤔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notify\n",
    "\n",
    "notify_table = pd.DataFrame({'time': [], 'url_type': []})\n",
    "with zp.ZipFile(zip_path) as zip_file:\n",
    "    folders = _aux_get_structure(zip_file)\n",
    "    \n",
    "    try:\n",
    "        with zip_file.open('about_you/notifications.json') as f:\n",
    "            jdata = json.loads(f.read())\n",
    "        \n",
    "            type_reg = re.compile(r'(https://www.facebook.com/)(.+?)([/.])(.*)')    \n",
    "            \n",
    "            times = []\n",
    "            types = []\n",
    "            \n",
    "            print(len(jdata['notifications']))\n",
    "            \n",
    "            for i in jdata['notifications']:\n",
    "                times.append(i['timestamp'])\n",
    "                try:\n",
    "                    types.append(re.findall(type_reg, i['href'])[0][1])\n",
    "                except:\n",
    "                    print(i['href'])\n",
    "                    types.append(None)\n",
    "            notify_table = pd.DataFrame({'time': times, 'url_type': types})\n",
    "            notify_table.time = pd.to_datetime(notify_table.time, unit='s')\n",
    "    except Exception as e:\n",
    "        print(\"Rollercoaster\", e)\n",
    "    \n",
    "notify_table\n",
    "not_type = notify_table.groupby(notify_table.url_type).count().time\n",
    "\n",
    "list(not_type)\n",
    "\n",
    "type_chart = pygal.Pie()\n",
    "for i, t in zip(not_type.index, list(not_type)):\n",
    "    type_chart.add(i, t)\n",
    "    \n",
    "type_chart\n",
    "\n",
    "\n",
    "# graph by hours\n",
    "by_hour = notify_table.groupby(notify_table.time.dt.hour)\n",
    "total = len(notify_table)\n",
    "by_hour = dict(by_hour.count().time)\n",
    "index = range(24)\n",
    "values = [(by_hour[i] / total) * 100 if i in by_hour else 0 for i in index]\n",
    "\n",
    "radar_chart = pygal.Radar(show_legend=False, fill=True, height=800)\n",
    "radar_chart.x_labels = index\n",
    "radar_chart.add('', values)\n",
    "\n",
    "                \n",
    "# graph by weekday\n",
    "by_day = notify_table.time.dt.dayofweek.value_counts()\n",
    "by_day = by_day / by_day.sum() * 100\n",
    "\n",
    "empty_series = pd.Series([0]*7, list(range(7)))\n",
    "by_day = by_day.add(empty_series, fill_value=0)\n",
    "\n",
    "days_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "pie_chart = pygal.Pie()\n",
    "for day, count in zip(days_names, round(by_day, 2)):\n",
    "    pie_chart.add(day, count)\n",
    "pie_chart\n",
    "notify_table == None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"https://www.facebook.com/stories/123133715513834/?source=notification\"\n",
    "type_reg = re.compile(r'(https://www.facebook.com/)(.+?)(/)(.*)')\n",
    "re.findall(type_reg, s)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
